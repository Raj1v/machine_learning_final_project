{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from itertools import product, combinations\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.compat import StringIO\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lo_MovieId = []\n",
    "lo_movie = []\n",
    "\n",
    "with open(\"data/ml-1m/movies.dat\", \"r\", encoding='windows-1252') as infile:\n",
    "    for line in infile:\n",
    "        movieId, movie, _ = line.split(\"::\")\n",
    "        lo_MovieId.append(movieId)\n",
    "        lo_movie.append(movie)\n",
    "\n",
    "arr_MovieId = np.array(lo_MovieId)[:, np.newaxis]\n",
    "arr_movie = np.array(lo_movie)[:, np.newaxis]\n",
    "MovieId2movie = np.concatenate((arr_MovieId, arr_movie), axis=1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1' 'Toy Story (1995)']\n",
      " ['2' 'Jumanji (1995)']\n",
      " ['3' 'Grumpier Old Men (1995)']\n",
      " ['4' 'Waiting to Exhale (1995)']\n",
      " ['5' 'Father of the Bride Part II (1995)']\n",
      " ['6' 'Heat (1995)']\n",
      " ['7' 'Sabrina (1995)']\n",
      " ['8' 'Tom and Huck (1995)']\n",
      " ['9' 'Sudden Death (1995)']\n",
      " ['10' 'GoldenEye (1995)']]\n",
      "3883\n"
     ]
    }
   ],
   "source": [
    "print(MovieId2movie[:10])\n",
    "len_movies = len(lo_movie)\n",
    "print(len_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3952\n"
     ]
    }
   ],
   "source": [
    "user2movies_ratings = defaultdict(dict)\n",
    "los_movieId = []\n",
    "\n",
    "with open(\"data/ml-1m/ratings.dat\", \"r\", encoding='utf-8') as infile:\n",
    "    for line in infile:\n",
    "        userId, movieId, rating = [int(el) for el in line.split(\"::\")[:3]]\n",
    "        user2movies_ratings[userId][movieId] = rating\n",
    "        los_movieId.append(movieId)\n",
    "\n",
    "max_movieId = max(los_movieId)\n",
    "print(max_movieId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_users = len(user2movies_ratings)\n",
    "len_movies = max_movieId\n",
    "\n",
    "init_matrix = np.zeros((len_users,len_movies))\n",
    "\n",
    "for user in user2movies_ratings:\n",
    "    for movie in user2movies_ratings[user]:\n",
    "        review = user2movies_ratings[user][movie]\n",
    "        init_matrix[user-1, movie-1] = review\n",
    "\n",
    "all_id_means =[]\n",
    "for i in range(len_users):\n",
    "    all_id_means.append([i+1, init_matrix[i][init_matrix[i] != 0].mean()])\n",
    "    \n",
    "arridmeans = np.array(all_id_means)\n",
    "\n",
    "matrix_and_means = np.hstack((arridmeans,init_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5. 5. 5. 4. 5. 4. 4. 4. 5. 4. 3. 3. 3. 4. 3. 4. 4. 5. 5. 5. 5. 4. 5. 3.\n",
      " 4. 4. 5. 5. 4. 4. 4. 5. 4. 5. 4. 4. 5. 4. 3. 3. 5. 4. 3. 4. 4. 4. 4. 5.\n",
      " 4. 5. 4. 4. 4.]\n"
     ]
    }
   ],
   "source": [
    "all_id_means =[]\n",
    "mean = True\n",
    "test = False\n",
    "if test:\n",
    "    for i in range(5):\n",
    "        id_mean = init_matrix[i][init_matrix[i] != 0].mean()\n",
    "        print(id_mean)\n",
    "        all_id_means.append([i, id_mean])\n",
    "        if mean:\n",
    "            init_matrix[i][init_matrix[i] !=0] -= id_mean\n",
    "\n",
    "    id_mean = np.array(all_id_means)\n",
    "    print(id_mean[:5])\n",
    "\n",
    "print(init_matrix[0][init_matrix[0] != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6040, 2) \n",
      "\n",
      "[5. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(arridmeans.shape,\"\\n\")\n",
    "print(matrix_and_means[:,2:][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(p,q):\n",
    "    d = sum(pi * qi for pi,qi in zip(p, q))\n",
    "    mag_p = math.sqrt(sum([pi**2 for pi in p]))\n",
    "    mag_q = math.sqrt(sum([qi**2 for qi in q]))\n",
    "    sim = d / ( mag_p * mag_q)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  4  9 16 25 36 49 64 81]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "A = np.arange(35).reshape(7,5)\n",
    "indarr = np.array([0,1,2,3,4,5])\n",
    "b=A!=5\n",
    "bb = A[np.all(b, axis=1)]\n",
    "bb[2] = np.zeros((5))\n",
    "bb[4] = np.zeros(5)\n",
    "bb[1][3] = 0\n",
    "bb[1][2] = 0\n",
    "bb[1][4] = 0\n",
    "\n",
    "B = np.arange(10)\n",
    "C = np.arange(10)[3:6]\n",
    "D = np.setdiff1d(B,C)\n",
    "E = np.flip(B.reshape(5,2))\n",
    "sbyrow2 = np.argsort(E.T[0, :])\n",
    "\n",
    "print(B * B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        recommendations = []\n",
    "        for movie in self.targetmovies:\n",
    "            weigthed_scores = []\n",
    "            similarities = []\n",
    "            for user, sim in self.neighbourhood:\n",
    "                if movieId in self.user_ratings[user]:\n",
    "                    weigthed_scores.append(sim * self.user_ratings[user][movie])\n",
    "                    similarities.append(sim)\n",
    "                    recommendations[movie] = sum(weigthed_scores) / sum(similarities)\n",
    "        self.recommendations = recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighbour(object):\n",
    "    def __init__(self, k = 50, sim_treshold = 0, measure = \"cosine\", show = True, mean= True):\n",
    "        self.k = k\n",
    "        self.sim_treshold = sim_treshold\n",
    "        self.smeasure = measure\n",
    "        self.fmeasure = None\n",
    "        self.data = None\n",
    "        self.MovieId = None\n",
    "        self.targetid = None\n",
    "        self.compid = None\n",
    "        self.neighbourhood = None\n",
    "        self.prediction = None\n",
    "        self.targetmovies = None\n",
    "        self.mean = mean\n",
    "        self.show = show\n",
    "        \n",
    "        \n",
    "    def init_data(self, data):\n",
    "        #Take the input data matrix and add two new columns at the start, the first with the user id, the second with \n",
    "        #this user's mean. Also if we want all our data to be normalized and mean=True, we do that here as well\n",
    "        new_data = data.copy()\n",
    "        all_id_means =[]\n",
    "        for i in range(len(new_data)):\n",
    "            id_mean = new_data[i][new_data[i] != 0].mean()\n",
    "            all_id_means.append([i+1, id_mean])\n",
    "            if self.mean:\n",
    "                new_data[i][new_data[i] !=0] -= id_mean\n",
    "        \n",
    "        id_mean = np.array(all_id_means)\n",
    "        self.data = np.hstack((id_mean,new_data))\n",
    "        \n",
    "    def get_neighbourhood(self):\n",
    "        #Get all the movies that your target id has rated without the movies you want to target\n",
    "        targetid_movies = np.setdiff1d((np.where(self.data[self.targetid] !=0)[0] -1),self.targetmovies) + 1\n",
    "        \n",
    "        #Get an array of all the ratings of your target\n",
    "        targetid_ratings = self.data[self.targetid][targetid_movies]\n",
    "        \n",
    "        #Take only the targeted movies\n",
    "        only_targets = self.data.T[targetid_movies].T\n",
    "        \n",
    "        #Be sure to take the data from only_targets[:,2:] as everyone has their id and mean in the first two columns\n",
    "        no_empty_ids = only_targets[np.any(only_targets[:,2:]!=0,axis=1)]\n",
    "        not_user = no_empty_ids[no_empty_ids[:,0]!= self.targetid+1]\n",
    "        \n",
    "        if self.show:\n",
    "            print(\"Shape of targetid_movies\",targetid_movies.shape,\"\\n\")\n",
    "            print(\"Shape of targetid_ratings\",targetid_ratings.shape,\"\\n\")\n",
    "            print(\"Shape of only_targets\",only_targets.shape,\"\\n\")\n",
    "            print(\"Shape of no_empty_ids\",no_empty_ids.shape,\"\\n\")\n",
    "            print(\"Shape of not_user\",not_user.shape,\"\\n\")\n",
    "\n",
    "        #Get the Matrix of users who have at least the sim_treshold amount of ratings        \n",
    "        satisfy_n = []\n",
    "        for i in range(len(not_user)):\n",
    "            if len(not_user[i][not_user[i] != 0]) <= self.sim_treshold:\n",
    "                continue\n",
    "            satisfy_n.append(not_user[i])\n",
    "        final_matrix = np.array(satisfy_n)\n",
    "        \n",
    "        if self.show:\n",
    "            print(\"Shape of final_matrix is:\",final_matrix.shape,\"\\n\")\n",
    "        \n",
    "        #Now calculate the similarity for each id that has at least sim_treshold movies in common\n",
    "        sim_and_id = []\n",
    "        for compid in final_matrix:\n",
    "            comp_movies = np.where(compid !=0)[0]\n",
    "            target_ratings = targetid_ratings[comp_movies][2:]\n",
    "            comp_ratings = compid[comp_movies][2:]\n",
    "            sim = self.fmeasure(target_ratings, comp_ratings)\n",
    "            sim_and_id.append([compid[0], sim])\n",
    "\n",
    "        sim_and_id = np.array(sim_and_id)\n",
    "        \n",
    "        if self.show:\n",
    "            print(\"Shape of sim_and_id:\",sim_and_id.shape,\"\\n\")\n",
    "            print(\"Example of sim_and_id is:\",sim_and_id[:5],\"\\n\")\n",
    "            \n",
    "        #We define our neighbourhood to be the first k highest sims, with their ID's in the first column                       \n",
    "        self.neighbourhood = sim_and_id[np.flip(np.argsort(sim_and_id.T[1, :]))][:self.k]\n",
    "\n",
    "    def get_recommendations(self):\n",
    "        #Get the users who where in this neighbourhood, make the type int so we can use indexing\n",
    "        top_k_users = self.neighbourhood.T[0].astype(int)\n",
    "        \n",
    "        #Take the movie_ratings our target had, only used if show = True\n",
    "        target_movie_ratings = self.data[self.targetid][self.targetmovies+1]\n",
    "        \n",
    "        #Takes the matrix of neighbour_ratings, for all our k users we get their scores regarding the target_movies\n",
    "        neighbour_ratings = self.data[top_k_users][:,self.targetmovies+1]\n",
    "\n",
    "        if self.show:\n",
    "            print(\"Targetuser has these ratings: \",target_movie_ratings,\"\\n\")\n",
    "            print(\"top_k_users has shape: \",top_k_users.shape,\"\\n\")\n",
    "            print(\"neighbour_ratings[:5] looks like: \",neighbour_ratings[:5],\"\\n\")\n",
    "        \n",
    "        #Calculate the predictions for each movie\n",
    "        predictions = []\n",
    "        for movie in neighbour_ratings.T:\n",
    "            users_rated = np.where(movie !=0)[0]\n",
    "            sims_users = self.neighbourhood.T[1][users_rated]\n",
    "            weighted_scores = movie[users_rated] * sims_users\n",
    "            print(sum(weighted_scores),sum(sims_users))\n",
    "            predict = sum(weighted_scores) / sum(sims_users)\n",
    "            predictions.append(predict)\n",
    "          \n",
    "        predictions = np.array(predictions)  \n",
    "        self.predictions = predictions\n",
    "        if self.mean:\n",
    "            self.predictions = predictions + self.data[self.targetid][1]\n",
    "      \n",
    "        if self.show:\n",
    "            print(\"predictions looks like \",predictions)\n",
    "            \n",
    "    def get_ratings(self, targetid, targetmovies):\n",
    "        \"\"\"Combines get_neighbourhood and get_recommendations to get the final predicted score per movie\"\"\"\n",
    "        #As we are mostly working with indexes subtracted the 1 from a movie ID here, makes our life easier in the future\n",
    "        self.targetid = targetid - 1\n",
    "        self.targetmovies = targetmovies\n",
    "        \n",
    "        \n",
    "        self.sim_measure()\n",
    "        self.get_neighbourhood()\n",
    "        self.get_recommendations()\n",
    "        \n",
    "        #Return an array of the movies and there respected score\n",
    "        return np.array([self.targetmovies,self.predictions]).T\n",
    "    \n",
    "          \n",
    "    def euclidean_similarity(self, p, q):\n",
    "        dist = math.sqrt(sum((pi-qi)**2 for pi,qi in zip(p, q)))\n",
    "        sim = 1 / (1+dist)\n",
    "        return sim   \n",
    "        \n",
    "    def manhattan_similarity(self, p, q):\n",
    "        dist = sum(np.abs(pi-qi) for pi,qi in zip(p, q))\n",
    "        sim = 1 / (1+dist)\n",
    "        return sim    \n",
    "\n",
    "    def cosine_similarity(self, p, q):\n",
    "        d = sum(pi * qi for pi,qi in zip(p, q))\n",
    "        mag_p = math.sqrt(sum([pi**2 for pi in p]))\n",
    "        mag_q = math.sqrt(sum([qi**2 for qi in q]))\n",
    "        sim = d / ( mag_p * mag_q)\n",
    "        return sim\n",
    "    \n",
    "    def pearson_correlation(self, p, q):\n",
    "    # this code does not scale well to large datasets. In the following, we rely on \n",
    "    # scipy.spatial.distance.correlation() to compute long vectors\n",
    "        if len(self.p) > 99:\n",
    "            return 1 - distance.correlation(p,q)        \n",
    "        \n",
    "        p_mean = sum(self.p) / len(p)\n",
    "        p_deviations = [(pi-p_mean) for pi in p]\n",
    "        \n",
    "        q_mean = sum(self.q) / len(q)\n",
    "        q_deviations = [(qi-q_mean) for qi in q]\n",
    "        \n",
    "        cov = sum(pde * qd for pde,qd in zip(p_deviations, q_deviations))\n",
    "            \n",
    "        sds_product = math.sqrt(sum((pder)**2 for pder in p_deviations) * sum((qd)**2 for qd in q_deviations))\n",
    "        \n",
    "        if sds_product != 0:\n",
    "            r = cov / sds_product\n",
    "        else:\n",
    "            r = 0\n",
    "        return r\n",
    "    \n",
    "    def jaccard_sets(self, p, q): \n",
    "        intersection_cardinality = len(set(p).intersection(set(q)))\n",
    "        union_cardinality = len(set(p).union(set(q)))\n",
    "        sim = intersection_cardinality / union_cardinality\n",
    "        return sim\n",
    "    \n",
    "    def jaccard_binary(self, p, q):\n",
    "        # only works for binary vectors! Binarize your vectors first\n",
    "        m_11, m_01, m_10 = 0, 0, 0\n",
    "        for pi, qi in zip(p, q):\n",
    "    \n",
    "            if pi == 1:\n",
    "                if qi == 1:\n",
    "                    m_11 += 1\n",
    "                else:\n",
    "                    m_10 += 1\n",
    "                    \n",
    "            elif qi == 1:\n",
    "                m_01 += 1\n",
    "        \n",
    "        sim = m_11 / (m_10 + m_01 + m_11) \n",
    "        return sim\n",
    "    \n",
    "    def sim_measure(self):\n",
    "        all_measures = {\"euclidean\" : self.euclidean_similarity,\n",
    "                        \"manhatten\" : self.manhattan_similarity,\n",
    "                        \"cosine\": self.cosine_similarity, \n",
    "                        \"correlation\": self.pearson_correlation,\n",
    "                        \"jaccardset\" : self.jaccard_sets,\n",
    "                        \"jaccardbin\" : self.jaccard_binary}\n",
    "        if self.smeasure in all_measures.keys():\n",
    "            self.fmeasure = all_measures[self.smeasure]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "KNN = KNearestNeighbour(k = 50, sim_treshold = 20, measure = \"cosine\", show = True, mean= True)\n",
    "data = KNN.init_data(init_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of targetid_movies (981,) \n",
      "\n",
      "Shape of targetid_ratings (981,) \n",
      "\n",
      "Shape of only_targets (6040, 981) \n",
      "\n",
      "Shape of no_empty_ids (6039, 981) \n",
      "\n",
      "Shape of not_user (6038, 981) \n",
      "\n",
      "Shape of final_matrix is: (4762, 981) \n",
      "\n",
      "Shape of sim_and_id: (4762, 2) \n",
      "\n",
      "Example of sim_and_id is: [[ 2.         -0.09280244]\n",
      " [ 3.         -0.10080074]\n",
      " [ 4.          0.13154182]\n",
      " [ 5.          0.44460952]\n",
      " [ 6.         -0.30385353]] \n",
      "\n",
      "Targetuser has these ratings:  [ 0.92668024 -1.07331976 -0.07331976] \n",
      "\n",
      "top_k_users has shape:  (50,) \n",
      "\n",
      "neighbour_ratings[:5] looks like:  [[ 0.          0.          1.025     ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 1.4893617   0.          0.        ]\n",
      " [ 0.71546961 -0.28453039  0.        ]\n",
      " [ 1.02745995  1.02745995  0.02745995]] \n",
      "\n",
      "12.387433096261582 15.784342411761571\n",
      "6.468704031305176 12.317806250141079\n",
      "2.2117307302096343 14.006205190639104\n",
      "predictions looks like  [0.78479247 0.52515066 0.15791078]\n",
      "[[ 593.            3.85811223]\n",
      " [1198.            3.59847041]\n",
      " [1270.            3.23123053]]\n"
     ]
    }
   ],
   "source": [
    "test1 = KNN.get_ratings(4447, np.array([593, 1198, 1270]))\n",
    "print(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 593.         1198.         1270.        ]\n",
      " [   3.85811223    3.59847041    3.23123053]]\n"
     ]
    }
   ],
   "source": [
    "print(test1.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
